{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#g-mind","title":"G-Mind","text":"<p>G-Mind is a developing project aimed at becoming a lightweight framework for integrating deep learning capabilities into projects developed using the Godot Engine. With G-Mind, the goal is to facilitate the incorporation of machine learning functionalities into Godot applications.</p> <p>Check out the installation section for further information on how to set up the project, or explore the demos to see examples of how to use G-Mind in your Godot project</p> <p>Note</p> <p>This project is under active development, and many things could change from one moment to the next</p> <p>Warning</p> <p>This framework does not currently support parallelization or acceleration by CPU or GPU. The developer is working on adding these features</p>"},{"location":"dense/","title":"Dense","text":"<p>SOURCE CODE</p> <p>The Dense class represents a fully connected layer in a neural network</p> \\[y = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\\] <p>This applies a linear transformation</p> Properties in_features The number of input features. Type: i8 out_features The number of output features. Type: i8 inputs An PackedFloat32Array of input values outputs An PackedFloat32Array of output values weights An Array of PackedFloat32Arrays, representing the weight matrix biases An PackedFloat32Array of bias values gradients_w An Array of PackedFloat32Arrays, representing the gradients of the weights gradients_b An PackedFloat32Array of gradients of the biases Methods func() create() Initializes a new instance of the Dense class to_string() Returns a string representation of the Dense instance apply_gradients() forward() calculate_gradient_norm() normalize_gradients() set_randf_weights_bias_and_zero_gradients() gradients_2_zero() calculate_derivative() derivative_respect_inputs() derivative_respect_weights()"},{"location":"dense/#densecreate","title":"Dense.create()","text":"<p>Initializes a new instance of the Dense class</p> <pre><code>    #[func]\n    fn create(in_features_: i8, out_features_: i8) -&gt; Gd&lt;Dense&gt; {\n        Gd::from_init_fn(|base| {\n            let mut dense_instance = Self {\n                in_features: in_features_,\n                out_features: out_features_,\n\n                inputs: PackedFloat32Array::new(),\n                outputs: PackedFloat32Array::new(),\n\n                weights: array![],\n                biases: PackedFloat32Array::new(),\n\n                gradients_w: array![],\n                gradients_b: PackedFloat32Array::new(),\n\n                base,\n            };\n            dense_instance.set_randf_weights_bias_and_zero_gradients();\n\n            dense_instance //return\n        })\n    }\n</code></pre> Args in_features_ The number of input features out_features_ The number of output features Return A new instance of the Dense class"},{"location":"dense/#example","title":"Example","text":"<pre><code>var dense = Dense.create(2, 15)\n</code></pre>"},{"location":"dense/#denseapply_gradients","title":"Dense.apply_gradients()","text":"<p>Applies the gradients to the <code>self.weights</code> and optionally sets the gradients to zero</p> <pre><code>    #[func]\n    fn apply_gradients(&amp;mut self, lr: f32, clean_grad: bool) {\n        // TODO: The line \"self.gradients_w.get(node_out_index).fill(0.)\" doesn't work because \"get\" returns the value of an index and doesn't function like a \"slice.\"\n        // I've also encountered difficulty transforming a \"vec\" into a \"Godot array.\"\n        // Consequently, I found it necessary to create the variables:\n        // [zero_layer_gradients_w, zero_layer_gradients_b, zero_node_gradients_w, zero_node_gradients_b].\n        // I'll refine this as soon as I have the time and opportunity.\n\n        let mut new_layer_weights: Array&lt;PackedFloat32Array&gt; = array![];\n        let mut new_layer_biases = PackedFloat32Array::new();\n\n        let mut zero_layer_gradients_w: Array&lt;PackedFloat32Array&gt; = array![];\n        let mut zero_layer_gradients_b = PackedFloat32Array::new();\n\n        for node_out_index in 0..self.weights.len() {\n            let node_weights: PackedFloat32Array = self.weights.get(node_out_index);\n\n            let mut new_node_weights = PackedFloat32Array::new();\n            let mut new_node_biases = 0.;\n\n            let mut zero_node_gradients_w = PackedFloat32Array::new();\n            let mut zero_node_gradients_b = 0.;\n\n            for node_in_index in 0..node_weights.len() {\n                let weight = self.weights.get(node_out_index).get(node_in_index);\n                let gradient_w = self.gradients_w.get(node_out_index).get(node_in_index);\n\n                new_node_weights.push( weight - (gradient_w * lr) );\n                zero_node_gradients_w.push(0.);\n            };\n\n            let bias = self.biases.get(node_out_index);\n            let gradient_b = self.gradients_b.get(node_out_index);\n\n            new_node_biases = bias - (gradient_b * lr);\n            zero_node_gradients_b = 0.;\n\n\n            new_layer_weights.push(new_node_weights);\n            new_layer_biases.push(new_node_biases);\n\n            zero_layer_gradients_w.push(zero_node_gradients_w);\n            zero_layer_gradients_b.push(zero_node_gradients_b);\n\n            // if clean_grad == true {\n            //     self.gradients_w.get(node_out_index).fill(0.)\n\n            // }\n        };\n\n        self.weights = new_layer_weights;\n        self.biases = new_layer_biases;\n\n        if clean_grad == true {\n            self.gradients_w = zero_layer_gradients_w;\n            self.gradients_b = zero_layer_gradients_b;\n        }\n\n\n        // if clean_grad == true {\n        //     self.gradients_b.fill(0.);\n        // }\n\n    }\n</code></pre> Args lr The learning rate clean_grad A boolean indicating whether to set the gradients to zero after applying them Return null"},{"location":"dense/#example_1","title":"Example","text":""},{"location":"dense/#denseforward","title":"Dense.forward()","text":"<p>Computes the output of the Dense layer for the given input</p> <pre><code>    #[func]\n    fn forward(&amp;mut self, x:PackedFloat32Array,) -&gt; PackedFloat32Array {\n        assert!(x.len() == self.in_features as usize, \"Error: The size of the input data doesn't match the expected input features for the layer.\");\n\n        self.inputs = x;\n        self.outputs = PackedFloat32Array::new();\n\n        let mut output = PackedFloat32Array::new();\n\n        for node_weights_index in 0..self.weights.len() {\n            let node_weights = self.weights.get(node_weights_index);\n            let mut node_output: f32 = 0.;\n\n            for weight_index in 0..node_weights.len() {\n                let weight: f32 = node_weights.get(weight_index);\n\n                node_output += self.inputs.get(weight_index) * weight;\n            }\n            node_output += self.biases.get(node_weights_index);\n\n            output.push(node_output);\n        }\n        self.outputs = output.clone();\n        output\n    }\n</code></pre> Args x An PackedFloat32Array of input values Return An PackedFloat32Array of output values"},{"location":"dense/#example_2","title":"Example","text":""},{"location":"dense/#densecalculate_gradient_norm","title":"Dense.calculate_gradient_norm()","text":"<p>Calculates the norm of the gradients</p> <pre><code>    #[func]\n    fn calculate_gradient_norm(&amp;mut self) -&gt; f32 {\n        let mut grad_norm: f32 = 0.;\n\n        for i in 0..self.gradients_w.len(){\n            let node_grad_weights: PackedFloat32Array = self.gradients_w.get(i);\n\n            for j in 0..node_grad_weights.len() {\n                let grad_weight: f32 = node_grad_weights.get(j);\n\n                grad_norm += grad_weight * grad_weight;\n\n            }\n\n            let grad_bias: f32 = self.gradients_b.get(i);\n\n            grad_norm += grad_bias * grad_bias;\n\n        }\n\n        grad_norm\n\n    }\n</code></pre> Args None Return The norm of the gradients as a <code>f32</code> value"},{"location":"dense/#example_3","title":"Example","text":""},{"location":"dense/#densenormalize_gradients","title":"Dense.normalize_gradients()","text":"<p>TODO</p> <pre><code>    #[func]\n    fn normalize_gradients(&amp;mut self, factor: f32,) {\n        let mut norm_grad_weights: Array&lt;PackedFloat32Array&gt; = Array::new();\n        let mut norm_grad_biases: PackedFloat32Array = PackedFloat32Array::new();\n\n        for i in 0..self.gradients_w.len() {\n            let node_grad_weights: PackedFloat32Array = self.gradients_w.get(i);\n\n            let mut norm_row_grads: PackedFloat32Array = PackedFloat32Array::new();  //row of the weights\n\n            for j in 0..node_grad_weights.len() {\n                let weight_grad: f32 = node_grad_weights.get(j);\n\n                norm_row_grads.push(factor * weight_grad);\n\n            }\n            let bias_grad: f32 = self.gradients_b.get(i);\n\n            norm_grad_weights.push(norm_row_grads);\n            norm_grad_biases.push(factor * bias_grad)\n\n        }\n        self.gradients_w = norm_grad_weights;\n        self.gradients_b = norm_grad_biases;\n    }\n</code></pre> Args factor Return null"},{"location":"dense/#example_4","title":"Example","text":""},{"location":"dense/#denseset_randf_weights_bias_and_zero_gradients","title":"Dense.set_randf_weights_bias_and_zero_gradients()","text":"<p>TODO</p> <pre><code>    #[func]\n    fn set_randf_weights_bias_and_zero_gradients(&amp;mut self) {\n        let mut rng = RandomNumberGenerator::new_gd();\n        // rng.set_seed(4555);\n        rng.randomize();\n        // let seed = rng.get_seed();\n\n        for out_features_index in 0..self.out_features {\n            let mut node_out_weights: PackedFloat32Array = PackedFloat32Array::new();\n            let mut row_gradients: PackedFloat32Array = PackedFloat32Array::new();\n\n            for in_feature_index in 0..self.in_features {\n                node_out_weights.push(rng.randf_range(-1., 1.));\n                row_gradients.push(0.);\n            }\n\n            self.weights.push(node_out_weights);\n            self.biases.push(rng.randf_range(-1., 1.));\n\n            self.gradients_w.push(row_gradients);\n            self.gradients_b.push(0.)\n        }\n\n    }\n</code></pre> Args Return"},{"location":"dense/#example_5","title":"Example","text":""},{"location":"dense/#densegradients_2_zero","title":"Dense.gradients_2_zero()","text":"<p>TODO</p> <pre><code>    #[func]\n    fn gradients_2_zero(&amp;mut self) {\n        let mut zero_weights_grad: Array&lt;PackedFloat32Array&gt; = Array::new();\n        let mut zero_biases_grad: PackedFloat32Array = PackedFloat32Array::new();\n\n        for out_features_index in 0..self.out_features {\n            let mut row_gradients: PackedFloat32Array = PackedFloat32Array::new();\n\n            for in_features_index in 0..self.in_features {\n                row_gradients.push(0.);\n\n            }\n            zero_weights_grad.push(row_gradients);\n            zero_biases_grad.push(0.)\n\n        }\n        self.gradients_w = zero_weights_grad;\n        self.gradients_b = zero_biases_grad;\n\n    }\n</code></pre> Args Return"},{"location":"dense/#example_6","title":"Example","text":""},{"location":"dense/#densecalculate_derivative","title":"Dense.calculate_derivative()","text":"<p>TODO</p> <pre><code>    #[func]\n    fn calculate_derivative(&amp;self) -&gt; Array&lt;PackedFloat32Array&gt; {\n        self.weights.clone()\n    }\n</code></pre> Args Return"},{"location":"dense/#example_7","title":"Example","text":""},{"location":"dense/#densederivative_respect_inputs","title":"Dense.derivative_respect_inputs()","text":"<p>TODO</p> <pre><code>    #[func]\n    fn derivative_respect_inputs(&amp;self) -&gt; Array&lt;PackedFloat32Array&gt; {\n        self.weights.clone()\n    }\n</code></pre> Args Return"},{"location":"dense/#example_8","title":"Example","text":""},{"location":"dense/#densederivative_respect_weights","title":"Dense.derivative_respect_weights()","text":"<p>TODO</p> <pre><code>    #[func]\n    fn derivative_respect_weights(&amp;self) -&gt; PackedFloat32Array {\n        self.inputs.clone()\n    }\n</code></pre> Args Return"},{"location":"dense/#example_9","title":"Example","text":""},{"location":"install/","title":"Install","text":""},{"location":"install/#install-from-releases","title":"Install from Releases","text":"<ol> <li> <p>Go to the G-Mind GitHub repository releases page and download the latest release package.</p> </li> <li> <p>Extract the downloaded package and copy the contents to the root of your Godot project <code>res://</code></p> </li> </ol>"},{"location":"install/#install-from-source-linux","title":"Install from source (Linux)","text":"<ol> <li> <p>Ensure you have Cargo installed on your system. </p> <pre><code>cargo --version\n</code></pre> </li> <li> <p>Clone the G-Mind repository: </p> <pre><code>git clone https://github.com/Jersonrn/G-Mind\n</code></pre> </li> <li> <p>Navigate to the Rust G-Mind project directory: </p> <pre><code>cd G-Mind/rust/\n</code></pre> </li> <li> <p>Build the Rust project in release mode:</p> <pre><code>cargo build --release\n</code></pre> </li> <li> <p>Copy the compiled <code>libg_mind.so</code> library to your Godot project's <code>lib/g_mind</code> directory: </p> <pre><code>cp target/release/libg_mind.so /path/to/your/godot/project/lib/g_mind\n</code></pre> </li> <li> <p>Copy the <code>scripts</code> directory to your Godot project's <code>lib/g_mind</code> directory:</p> <pre><code>cd ../..\ncp -r godot/scripts /path/to/your/godot/project/lib/g_mind\n</code></pre> </li> <li> <p>Create a <code>g_mind.gdextension</code> file in the root of your Godot project:</p> <p><pre><code>cd /path/to/your/godot/project\ntouch g_mind.gdextension\n</code></pre> Then, add the following content to the <code>g_mind.gdextension</code> file: <pre><code>[configuration]\nentry_symbol = \"gdext_rust_init\"\ncompatibility_minimum = 4.2\n\n[libraries]\nlinux.debug.x86_64 = \"res://lib/g_mind/libg_mind.so\"\nlinux.release.x86_64 = \"res://lib/g_mind/libg_mind.so\"\n</code></pre></p> </li> </ol>"},{"location":"mse_loss/","title":"MSELoss class","text":"<p>SOURCE CODE</p> <p>The MSE Loss is a measure of the average squeared difference between the predited values \\(y\\) and the target values \\(\\hat{y}\\)</p> \\[\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\] Properties outputs An float value representing the MSE loss y An Tensor containing the target values y_hat An Tensor containing the predicted values Methods new() Initializes a new instance of the MSELoss class _to_string() Converts the MSELoss object to a string representation forward() Calculates the mean squared error (MSE) between the predicted values and the target values calculate_derivative() Calculates the derivative of the MSE loss with respect to the predicted values"},{"location":"mse_loss/#mselossnew","title":"MSELoss.new()","text":"<p>Initializes a new instance of the MSELoss class</p> <pre><code>func _init():\n    pass\n</code></pre> Args None Return A new instance of the MSELoss class"},{"location":"mse_loss/#example","title":"Example","text":"<pre><code>var mse_loss = MSELoss.new()\n</code></pre>"},{"location":"mse_loss/#mselossforward","title":"MSELoss.forward()","text":"<p>Calculates the mean squared error (MSE) between the predicted values \\(\\hat{y}\\) and the target values \\(y\\)</p> <pre><code>func forward(y_hat_: Tensor, y_: Tensor) -&gt; Tensor:\n    self.y_hat = y_hat_; self.y = y_\n\n    assert(self.y_hat.size() == self.y.size(), \"The sizes of 'y_hat' and 'y' must be equal\")\n\n    var output:= Tensor.new([self.outputs], self.y_hat.grad_funcs.duplicate())\n    var loss: float = 0.0\n\n    for idx in range(len(self.y_hat.values)):\n        loss += ( self.y_hat.values[idx] - self.y.values[idx] ) ** 2\n\n    self.outputs = loss / y.size()\n\n    output.values = Array([self.outputs])\n    output.add_grad_func(self)\n\n    return output\n</code></pre> Args y_hat_ A Tensor containing the predicted values y_ A Tensor containing the target values Return A Tensor containing the average squared difference between <code>y_hat_</code> and <code>y_</code>"},{"location":"mse_loss/#example_1","title":"Example","text":"<pre><code>var y =     Tensor.new([0.3, 0.9, 0.3])\nvar y_hat = Tensor.new([0.9, 0.8, 0.3])\n\nvar mse_loss = MSELoss.new()\n\nvar loss = mse_loss.forward(y_hat, y)\n\nfunc _init():\n    print(loss)\n\n# Output: Tensor(values=[0.12333333333333])\n</code></pre>"},{"location":"mse_loss/#mselosscalculate_derivative","title":"MSELoss.calculate_derivative()","text":"<p>Calculates the derivative of the MSE Loss with respect to the predicted values. This is used during backpropagation</p> \\[ \\frac{\\partial \\text{MSE}}{\\partial \\hat{y}_i} = 2 (\\hat{y}_i - y_i) \\] <pre><code>func calculate_derivative() -&gt; Tensor:\n    var output := Tensor.new()\n\n    for idx in range(len(self.y_hat.values)):\n        output.append( 2 * (self.y_hat.values[idx] - self.y.values[idx]) )\n\n    return output\n</code></pre> <p>This method calculates the derivative for each individual predicted value, rather than the average derivative</p> \\[ \\frac{\\partial \\text{MSE}}{\\partial \\hat{y}} = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) \\] Args None Return A Tensor object containing the derivatives of the MSE loss with respect to each predicted value"},{"location":"mse_loss/#example_2","title":"Example","text":"<pre><code># No example provided\n</code></pre>"},{"location":"reduce_lr_on_plateau/","title":"ReduceLROnPlateau","text":"<p>SOURCE CODE</p> <p>The ReduceLROnPlateau technique dynamically reduces the learning rate during training when the model's performance stops improving</p> Properties patience An integer value that determines the number of epochs to wait before reducing the learning rate if the metric has not improved mode Specifies whether the metric being monitored is a maximization or minimization problem. The possible values are <code>Modes.MAX</code> for maximization and <code>Modes.MIN</code> for minimization factor A float value that determines the factor by which the learning rate will be reduced. The new learning rate will be the current learning rate multiplied by this factor threshold A float value that determines the minimum change in the monitored metric required to be considered an improvement num_bad_epochs An integer value that keeps track of the number of epochs where the metric has not improved cooldown An integer value that determines the number of epochs to wait before resuming normal operation after a learning rate reduction min_lr A float value that determines the minimum learning rate allowed. The learning rate will not be reduced below this value verbose A boolean value that determines whether to print messages about the learning rate reduction process cooldown_counter An integer value that keeps track of the current cooldown period best A float value that stores the best value of the monitored metric so far Methods new() is_better() in_cooldown() reset() step() worse()"},{"location":"reduce_lr_on_plateau/#reducelronplateaunew","title":"ReduceLROnPlateau.new()","text":"<p>Initializes a new instance of the ReduceLROnPlateau class</p> <pre><code>func _init(\n        _patience: int = 5,\n        _mode: String = \"max\",\n        _factor: float = 0.1,\n        _threshold: float = 0.1,\n        _num_bad_epochs: int = 0,\n        _cooldown: int = 0,\n        _min_lr: float = 0.000001,\n        _verbose: bool = false,\n    ):\n    self.patience = _patience\n    if _mode == \"max\":\n        self.mode = self.Modes.MAX\n    elif _mode == \"min\":\n        self.mode = self.Modes.MIN\n    else:\n        push_error(\"Unknown mode for scheduler ReduceLROnPlateau, please use 'max' or 'min'\")\n    self.factor = _factor\n    self.threshold = _threshold\n    self.num_bad_epochs = _num_bad_epochs\n    self.best = self.worse()\n    self.cooldown = _cooldown\n    self.min_lr = _min_lr\n    self.verbose = _verbose\n\n    self.cooldown_counter = self.cooldown\n</code></pre> Args _patience An integer value that determines the number of epochs to wait before reducing the learning rate if the metric has not improved _mode A string value that specifies whether the metric being monitored is a maximization or minimization problem. The possible values are \"max\" for maximization and \"min\" for minimization _factor A float value that determines the factor by which the learning rate will be reduced _threshold A float value that determines the minimum change in the monitored metric required to be considered an improvement _num_bad_epochs An integer value that keeps track of the number of epochs where the metric has not improved _cooldown An integer value that determines the number of epochs to wait before resuming normal operation after a learning rate reduction _min_lr A float value that determines the minimum learning rate allowed _verbose A boolean value that determines whether to print messages about the learning rate reduction process Return A new instance of the ReduceLROnPlateau class"},{"location":"reduce_lr_on_plateau/#example","title":"Example","text":"<pre><code>var scheduler = ReduceLROnPlateau.new(\n        5,          #patience\n        \"min\",      #mode\n        0.001,      #factor\n        0.0001,     #threshold\n        0,          #num_bad_epochs\n        0,          #cooldown\n        0.000001,   #min_lr\n        true        #verbose\n    )\n</code></pre>"},{"location":"reduce_lr_on_plateau/#reducelronplateauis_better","title":"ReduceLROnPlateau.is_better()","text":"<p>Checks if the current metric value is better than the best value so far, based on the specified mode (maximization or minimization)</p> <pre><code>func is_better(current):\n    if self.mode == self.Modes.MAX:\n        return current &gt; self.best + self.threshold\n    elif self.mode == self.Modes.MIN:\n        return current &lt; self.best - self.threshold\n    else:\n        push_error(\"Unknow mode for scheduler ReduceLROnPlateau\")\n</code></pre> Args current The current metric value Return A boolean value indicating whether the current metric value is better than the best value so far"},{"location":"reduce_lr_on_plateau/#example_1","title":"Example","text":""},{"location":"reduce_lr_on_plateau/#reducelronplateauin_cooldown","title":"ReduceLROnPlateau.in_cooldown()","text":"<p>Checks if the scheduler is currently in a cooldown period</p> <pre><code>func in_cooldown():\n    return self.cooldown_counter &gt; 0\n</code></pre> Args None Return A boolean value indicating whether the scheduler is in a cooldown period"},{"location":"reduce_lr_on_plateau/#example_2","title":"Example","text":""},{"location":"reduce_lr_on_plateau/#reducelronplateaureset","title":"ReduceLROnPlateau.reset()","text":"<p>Resets the <code>self.num_bad_epochs</code> and <code>self.cooldown_counter</code> properties</p> <pre><code>func reset():\n    self.num_bad_epochs = 0\n    self.cooldown_counter = 0\n</code></pre> Args None Return null"},{"location":"reduce_lr_on_plateau/#example_3","title":"Example","text":""},{"location":"reduce_lr_on_plateau/#reducelronplateaustep","title":"ReduceLROnPlateau.step()","text":"<p>Updates the learning rate based on the provided metric value and the current state of the scheduler</p> <pre><code>func step(metrics, lr) -&gt; float:\n    if self.in_cooldown():\n        self.cooldown_counter -= 1\n    else:\n        if self.is_better(metrics):\n            self.best = metrics\n            self.num_bad_epochs = 0\n        else:\n            self.num_bad_epochs += 1\n\n    if self.num_bad_epochs &gt; self.patience:\n        self.cooldown_counter = self.cooldown\n        self.num_bad_epochs = 0\n\n        var new_lr = lr - self.factor\n\n        if new_lr &lt; self.min_lr:\n            if verbose:\n                print(\"Min lr reached \", self.min_lr)\n            return self.min_lr\n\n        if verbose:\n            print(\"Reducing learning rate to \", new_lr)\n        return new_lr\n    else:\n        return lr\n</code></pre> Args metrics The current metric value lr The current learning rate Return A float with the updated learning rate"},{"location":"reduce_lr_on_plateau/#example_4","title":"Example","text":""},{"location":"reduce_lr_on_plateau/#reducelronplateauworse","title":"ReduceLROnPlateau.worse()","text":"<p>Returns the initial \"worst\" value for the monitored metric, based on the specified mode (maximization or minimization)</p> <pre><code>func worse() -&gt; float:\n    if self.mode == self.Modes.MAX:\n        return 0.0\n    else:\n        return 1000000.0\n</code></pre> Args None Return The initial \"worst\" value for the monitored metric"},{"location":"reduce_lr_on_plateau/#example_5","title":"Example","text":""},{"location":"sequential/","title":"Sequential class","text":"<p>SOURCE CODE</p> <p>This class applies a sequence of operations in a consecutive manner</p> Properties layers An Array that holds the operations to be applied Methods new() Creates a new Sequential object _to_string() Converts the Sequential object to a string representation clip_gradients() Clips the gradients of <code>self.layers</code> to a specified maximum norm forward() Applies a sequence of operations to a Tensor gradients_to_zero() Sets the gradients of all layers in <code>self.layers</code> to zero"},{"location":"sequential/#sequentialnew","title":"Sequential.new()","text":"<p>Creates a new Sequential object</p> <pre><code>func _init( layers_: Array = [] ):\n    self.layers = layers_\n</code></pre> Args layers_ An array of operations to be applied Return A Sequential object"},{"location":"sequential/#example","title":"Example","text":"<pre><code>var Sequential.new([\n    Dense.create(2, 15),\n    LeakyRelu.new(),\n])\n</code></pre>"},{"location":"sequential/#sequentialclip_gradients","title":"Sequential.clip_gradients()","text":"<p>Clips the gradients of <code>self.layers</code> to a specified maximum norm</p> <pre><code>func clip_gradients(max_norm: float = 1.0):\n    var norm = 0.\n\n    for layer in layers:\n        if \"gradients_w\" in layer:\n            norm += layer.calculate_gradient_norm()\n\n    if norm &gt; max_norm:\n        var factor = max_norm / norm\n\n        for layer in layers:\n            if \"gradients_w\" in layer:\n                layer.normalize_gradients(factor)\n</code></pre> Args max_norm The maximum allowed gradient norm. Gradients will be scaled down if the overall norm exceeds this value. Default is 1.0 Return null"},{"location":"sequential/#example_1","title":"Example","text":""},{"location":"sequential/#sequentialforward","title":"Sequential.forward()","text":"<p>Apply a sequence of operations to a Tensor</p> <pre><code>func forward(x: Tensor) -&gt; Tensor:\n    var xx: Tensor = x.duplicate()\n\n    for layer in self.layers:\n        xx.values = layer.forward(PackedFloat32Array(xx.values))\n        xx.add_grad_func(layer)\n\n    return xx\n</code></pre> Args x An Tensor to which the operations will be applied Return Tensor resulting from the sequence of operations"},{"location":"sequential/#example_2","title":"Example","text":""},{"location":"sequential/#sequentialgradients_to_zero","title":"Sequential.gradients_to_zero()","text":"<p>Sets the gradients of all layers in <code>self.layers</code> to zero</p> <pre><code>func gradients_to_zero():\n    for layer in self.layers:\n        if \"gradients_w\" in layer:\n            layer.gradients_2_zero()\n</code></pre> Args Return null"},{"location":"sequential/#example_3","title":"Example","text":""},{"location":"sequential/#sequentialstep","title":"Sequential.step()","text":"<p>Applies the gradients to the weights of all layers in <code>self.layers</code> and optionally sets the gradients to zero</p> <pre><code>#apply_gradients\nfunc step(learn_rate = 0.001, grad_to_zero: bool = false):\n    for layer in self.layers:\n        if \"gradients_w\" in layer:\n            layer.apply_gradients(learn_rate, grad_to_zero)\n</code></pre> Args learn_rate The learning rate to be used for updating the weights. Default is 0.001 grad_to_zero If <code>true</code>, the gradients of all layers will be set to zero after the update. Default is <code>false</code> Return null"},{"location":"sequential/#example_4","title":"Example","text":""},{"location":"tensor/","title":"Tensor class","text":"<p>SOURCE CODE</p> <p>The Tensor class is the most important part of the G-Mind framework. It stores values and gradient functions that will be applied to it during backpropagation.</p> Properties values An array of values stored in the Tensor grad_funcs An array of gradient functions that have been applied to the Tensor Methods new() _to_string() add_grad_func() append() backward() clear() clear_grad_funcs() get_shape() multiply() size() to_packedfloat32array() ones_like() zeros_like()"},{"location":"tensor/#tensornew","title":"Tensor.new()","text":"<p>Creates a new Tensor object</p> <pre><code>func _init(values_: Array = [], grad_funcs_: Array = []):\n    self.values = values_\n    self.grad_funcs = grad_funcs_\n</code></pre> Args values_ An array of values to be stored in the Tensor grad_funcs_ An array of gradient functions to be applied to the Tensor Return A new instance of the Tensor class"},{"location":"tensor/#example","title":"Example","text":"<pre><code>var tensor = Tensor.new([-1.4, 0.3, 0.9])\nprint(tensor)\n\n# Output: Tensor(values=[-1.4, 0.3, 0.9])\n</code></pre>"},{"location":"tensor/#tensoradd_grad_func","title":"Tensor.add_grad_func()","text":"<p>Adds a gradient function to the Tensor.</p> <pre><code>func add_grad_func(grad_func):\n    self.grad_funcs.append(grad_func)\n</code></pre> Args grad_func A gradient function to be added to the Tensor Return null"},{"location":"tensor/#example_1","title":"Example","text":"<pre><code>func  _init():\n    var tensor = Tensor.new([-1.4, 0.3, 0.9])\n    var sigmoid = Sigmoid.new()\n\n    var f32_array = tensor.to_packedfloat32array()\n\n    f32_array = sigmoid.forward(f32_array)\n\n    tensor = Tensor.new(\n            f32_array,\n            tensor.grad_funcs\n        )\n    tensor.add_grad_func(sigmoid)\n\n    print(tensor.values)\n    print(tensor.grad_funcs)\n\n# Output:\n# [0.19781611859798, 0.57444250583649, 0.7109494805336]\n# [Sigmoid()]\n</code></pre>"},{"location":"tensor/#tensorappend","title":"Tensor.append()","text":"<p>Adds a value to the Tensor</p> <pre><code>func append(x):\n    self.values.append(x)\n</code></pre> Args x A value to be added to the Tensor Return null"},{"location":"tensor/#example_2","title":"Example","text":"<pre><code># No example provided\n</code></pre>"},{"location":"tensor/#tensorbackward","title":"Tensor.backward()","text":"<p>Performs backpropagation on the Tensor</p> <pre><code>func backward(batch_size_):\n    assert(!self.grad_funcs.is_empty(),\"No 'grad_funcs' found for this Tensor\")\n\n    var factor := float(1./batch_size_)\n\n    var output: Tensor = self.grad_funcs[-1].calculate_derivative().ones_like()\n\n    for idx in range(len(self.grad_funcs) - 1, -1, -1):\n        var current_layer = self.grad_funcs[idx]\n\n        if \"gradients_w\" in current_layer:\n            #The length of the weights for each node in the current layer (len(self.weights[0 | 1 | ...n]))\n            #matches the number of output nodes from the prev layer, which is precisely what we require at this point.\n            var n_nodes_out_prev_layer = len(current_layer.weights[0])\n\n            var derivative_weights := Tensor.new( current_layer.derivative_respect_weights() )\n            var derivative_inputs := Tensor.new( current_layer.derivative_respect_inputs() )\n\n            var new_total_derivative: Array = []\n\n            for n_o_p_idx in range(n_nodes_out_prev_layer):\n                var new_node_derivative : float = 0.0\n\n                for n_o_c_idx in range(current_layer.out_features):\n                    var weight = derivative_inputs.values[n_o_c_idx][n_o_p_idx]\n                    var node_derivative = output.values[n_o_c_idx]\n\n                    new_node_derivative +=  weight * node_derivative \n\n                    #Updating weights gradients\n                    var a = current_layer.gradients_w\n                    current_layer.gradients_w[n_o_c_idx][n_o_p_idx] += ( output.values[n_o_c_idx] * derivative_weights.values[n_o_p_idx] ) * factor\n\n                new_total_derivative.append(new_node_derivative)\n\n            #Updating bias gradients\n            for n_o_idx in range(current_layer.out_features):\n                current_layer.gradients_b[n_o_idx] += ( 1 * output.values[n_o_idx] ) * factor\n\n            output.values = new_total_derivative\n        else:\n            var r = current_layer.calculate_derivative()\n            var derivative_layer: Tensor = current_layer.calculate_derivative()\n            output = output.multiply(derivative_layer)\n</code></pre> Args batch_size_ The batch size used for the backpropagation Return null"},{"location":"tensor/#example_3","title":"Example","text":"<pre><code>func  _init():\n    var tensor = Tensor.new([-1.4, 0.3, 0.9])\n    var sigmoid = Sigmoid.new()\n\n    var f32_array = tensor.to_packedfloat32array()\n\n    f32_array = sigmoid.forward(f32_array)\n\n    tensor = Tensor.new(\n            f32_array,\n            tensor.grad_funcs\n        )\n    tensor.add_grad_func(sigmoid)\n\n    print(tensor.values)\n    print(tensor.grad_funcs)\n\n# Output:\n# [0.19781611859798, 0.57444250583649, 0.7109494805336]\n# [Sigmoid()]\n</code></pre>"},{"location":"tensor/#tensorclear","title":"Tensor.clear()","text":"<p>Clears the Tensor.</p> <pre><code>func clear():\n    self.values.clear()\n    self.grad_funcs.clear()\n</code></pre> Args None Return null"},{"location":"tensor/#example_4","title":"Example","text":"<pre><code># No example provided\n</code></pre>"},{"location":"activation_functions/leaky_relu/","title":"Leaky ReLU","text":"<p>SOURCE CODE</p> <p>The Leaky ReLU (Rectified Linear Unit) is a mathematical function that is a variation of the standard ReLU activation function used in deep learning models. The Leaky ReLU takes a real-valued input and maps it to a value that is either the input itself (for positive values) or a small, non-zero value (for negative values).</p>"},{"location":"activation_functions/leaky_relu/#leakyreluforward","title":"LeakyRelu.forward()","text":"<p>Applies the Leaky ReLU function to each element of a PackedFloat32Array.</p> \\[\\begin{align*} f(x) = \\begin{cases}     x &amp; \\text{if } x \\geq 0 \\\\     \\alpha x &amp; \\text{if } x &lt; 0 \\end{cases} \\end{align*}\\] <pre><code>    func forward(xx: PackedFloat32Array) -&gt; PackedFloat32Array:\n        self.inputs = xx\n        var output: PackedFloat32Array = []\n\n        for x in xx: output.append(max(self.negative_slope * x, x))\n\n        return output\n</code></pre> Args x A PackedFloat32Array 1D Return A PackedFloat32Array with the same shape as x <p></p> \\[ LeakyReLU(x) = \\begin{cases} \\alpha x &amp; \\text{if } x &lt; 0 \\ x &amp; \\text{if } x \\geq 0 \\end{cases} \\]"},{"location":"activation_functions/leaky_relu/#leakyrelucalculate_derivative","title":"LeakyRelu.calculate_derivative()","text":"<p>The LeakyRelu.calculate_derivative() method computes the derivative of the leaky relu function for each element in the self.input array.</p> \\[ f'(x) = \\begin{cases}     \\alpha &amp; \\text{if } x &lt; 0 \\\\     1 &amp; \\text{if } x \\geq 0 \\end{cases} \\] <pre><code>func calculate_derivative() -&gt; Tensor:\n    var output := Tensor.new()\n\n    for x in self.inputs:\n        if x &gt;= 0:\n            output.append(1.0)\n        else:\n            output.append(self.negative_slope)\n\n    return output\n</code></pre> Args Return A Tensor with the same shape as self.inputs <p></p> \\[ LeakyReLU'(x) = \\begin{cases} \\alpha &amp; \\text{if } x &lt; 0 \\ 1 &amp; \\text{if } x \\geq 0 \\end{cases} \\]"},{"location":"activation_functions/leaky_relu/#examples","title":"Examples","text":"<pre><code>var x := PackedFloat32Array()\n\nvar leaky_relu = LeakyRelu.new(negative_slope=0.01)\n\nfunc _ready():\n    x = leaky_relu.forward(x)\n    var derivative = leaky_relu.calculate_derivative()\n\n    print(derivative.values)\n</code></pre>"},{"location":"activation_functions/sigmoid/","title":"Sigmoid","text":"<p>SOURCE CODE</p> <p>The sigmoid function, also known as the logistic function, is a mathematical function that takes a real-valued input and maps it to a value between 0 and 1. The sigmoid function is commonly used in machine learning and neural networks, particularly in the context of binary classification problems.</p>"},{"location":"activation_functions/sigmoid/#sigmoidforward","title":"Sigmoid.forward()","text":"<p>Applies the Sigmoid function to each element of a PackedFloat32Array.</p> \\[\\text{Sigmoid.forward}(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}}\\] <pre><code>    func forward(xx: PackedFloat32Array) -&gt; PackedFloat32Array:\n        self.inputs = xx\n        var output: PackedFloat32Array = []\n\n        for x  in xx: output.append( 1 / (1 + exp(-x)) )\n\n        return output\n</code></pre> Args x A PackedFloat32Array 1D Return A PackedFloat32Array with the same shape as x <p></p> \\[\\text{For } x \\in (-\\infty, \\infty)\\text{, } \\mathrm{sigmoid}(x) \\in (0, 1)\\]"},{"location":"activation_functions/sigmoid/#sigmoidcalculate_derivative","title":"Sigmoid.calculate_derivative()","text":"<p>The Sigmoid.calculate_derivative() method computes the derivative of the sigmoid function for each element in the self.input array.</p> \\[\\frac{d\\sigma(x)}{dx} = \\sigma(x)(1 - \\sigma(x))\\] <pre><code>func calculate_derivative() -&gt; Tensor:\n    var output := Tensor.new()\n\n    for x in self.inputs: output.append(exp(-x) * (1 + exp(-x)) ** -2)\n\n    return  output\n</code></pre> Args Return A Tensor with the same shape as self.inputs <p></p> \\[\\sigma'(x)\\]"},{"location":"activation_functions/sigmoid/#examples","title":"Examples","text":"<pre><code>var x := PackedFloat32Array()\n\nvar sigmoid = Sigmoid.new()\n\nfunc _ready():\n    x = sigmoid.forward(x)\n    var derivative = sigmoid.calculate_derivative()\n\n    print(derivative.values)\n</code></pre>"}]}